{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "431475e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow_docs.vis import embed\n",
    "from tensorflow import keras\n",
    "from imutils import paths\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import imageio\n",
    "import cv2\n",
    "import os\n",
    "\n",
    "from rl_with_videos.preprocessors.convnet import convnet_preprocessor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dec51d35",
   "metadata": {},
   "source": [
    "Use UCF101 dataset as examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8cc55fff",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SIZE = 48\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 10\n",
    "\n",
    "MAX_SEQ_LENGTH = 20\n",
    "NUM_FEATURES = 2048\n",
    "\n",
    "TRAINING_FILE = \"C:/nyu/DRL/final_project/dataset/UCF101/train.csv\"\n",
    "TESTING_FILE = \"C:/nyu/DRL/final_project/dataset/UCF101/test.csv\"\n",
    "\n",
    "LABELS_CLASS = ['CricketShot', 'PlayingCello', 'Punch', 'ShavingBeard', 'TennisSwing']\n",
    "NUM_CLASSES = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a276ac23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total videos for training: 594\n",
      "Total videos for testing: 224\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>video_name</th>\n",
       "      <th>tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>190</th>\n",
       "      <td>v_PlayingCello_g18_c07.avi</td>\n",
       "      <td>PlayingCello</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>264</th>\n",
       "      <td>v_Punch_g11_c07.avi</td>\n",
       "      <td>Punch</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>255</th>\n",
       "      <td>v_Punch_g10_c04.avi</td>\n",
       "      <td>Punch</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>378</th>\n",
       "      <td>v_ShavingBeard_g10_c06.avi</td>\n",
       "      <td>ShavingBeard</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>269</th>\n",
       "      <td>v_Punch_g12_c05.avi</td>\n",
       "      <td>Punch</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>v_CricketShot_g10_c07.avi</td>\n",
       "      <td>CricketShot</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>v_CricketShot_g08_c07.avi</td>\n",
       "      <td>CricketShot</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>254</th>\n",
       "      <td>v_Punch_g10_c03.avi</td>\n",
       "      <td>Punch</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>188</th>\n",
       "      <td>v_PlayingCello_g18_c05.avi</td>\n",
       "      <td>PlayingCello</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>v_CricketShot_g22_c02.avi</td>\n",
       "      <td>CricketShot</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     video_name           tag\n",
       "190  v_PlayingCello_g18_c07.avi  PlayingCello\n",
       "264         v_Punch_g11_c07.avi         Punch\n",
       "255         v_Punch_g10_c04.avi         Punch\n",
       "378  v_ShavingBeard_g10_c06.avi  ShavingBeard\n",
       "269         v_Punch_g12_c05.avi         Punch\n",
       "20    v_CricketShot_g10_c07.avi   CricketShot\n",
       "6     v_CricketShot_g08_c07.avi   CricketShot\n",
       "254         v_Punch_g10_c03.avi         Punch\n",
       "188  v_PlayingCello_g18_c05.avi  PlayingCello\n",
       "93    v_CricketShot_g22_c02.avi   CricketShot"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = pd.read_csv(TRAINING_FILE)\n",
    "test_df = pd.read_csv(TESTING_FILE)\n",
    "\n",
    "print(f\"Total videos for training: {len(train_df)}\")\n",
    "print(f\"Total videos for testing: {len(test_df)}\")\n",
    "\n",
    "train_df.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "60dc708c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_center_square(frame):\n",
    "    y, x = frame.shape[0:2]\n",
    "    min_dim = min(y, x)\n",
    "    start_x = (x // 2) - (min_dim // 2)\n",
    "    start_y = (y // 2) - (min_dim // 2)\n",
    "    return frame[start_y : start_y + min_dim, start_x : start_x + min_dim]\n",
    "\n",
    "\n",
    "def load_video(path, max_frames=0, resize=(IMG_SIZE, IMG_SIZE)):\n",
    "    cap = cv2.VideoCapture(path)\n",
    "    frames = []\n",
    "    try:\n",
    "        while True:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            frame = crop_center_square(frame)\n",
    "            frame = cv2.resize(frame, resize)\n",
    "            frame = frame[:, :, [2, 1, 0]]\n",
    "            frames.append(frame)\n",
    "\n",
    "            if len(frames) == max_frames:\n",
    "                break\n",
    "    finally:\n",
    "        cap.release()\n",
    "    return np.array(frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d6996d60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def build_feature_extractor():\n",
    "#     feature_extractor = keras.applications.InceptionV3(\n",
    "#         weights=\"imagenet\",\n",
    "#         include_top=False,\n",
    "#         pooling=\"avg\",\n",
    "#         input_shape=(IMG_SIZE, IMG_SIZE, 3),\n",
    "#     )\n",
    "#     preprocess_input = keras.applications.inception_v3.preprocess_input\n",
    "\n",
    "#     inputs = keras.Input((IMG_SIZE, IMG_SIZE, 3))\n",
    "#     preprocessed = tf.keras.layers.Lambda(\n",
    "#         lambda x: preprocess_input(x))(inputs)\n",
    "\n",
    "#     outputs = feature_extractor(preprocessed)\n",
    "#     return keras.Model(inputs, outputs, name=\"feature_extractor\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2a15f1d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\irohc\\anaconda3\\envs\\drl_proj\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "feature_extractor = keras.applications.InceptionV3(\n",
    "        weights=\"imagenet\",\n",
    "        include_top=False,\n",
    "        pooling=\"avg\",\n",
    "        input_shape=(IMG_SIZE, IMG_SIZE, 3),\n",
    ")\n",
    "feature_extractor.trainable = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fe993d22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature_extractor = build_feature_extractor()\n",
    "# feature_extractor.trainable = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7975f696",
   "metadata": {},
   "outputs": [],
   "source": [
    "video = keras.layers.Input(shape=(None,224,224,3),name='video_input')\n",
    "encoded_frame = keras.layers.TimeDistributed(keras.layers.Lambda(lambda x: feature_extractor(x)))(video)\n",
    "encoded_vid = keras.layers.LSTM(16, return_sequences=True)(encoded_frame)\n",
    "encoded_vid = keras.layers.LSTM(8, return_sequences=False)(encoded_vid)\n",
    "encoded_vid = keras.layers.Dense(8, activation='relu')(encoded_vid)\n",
    "outputs = keras.layers.Dense(5, activation='softmax')(encoded_vid)\n",
    "model = keras.models.Model(inputs=[video],outputs=outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "86ceb059",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "video_input (InputLayer)     (None, None, 224, 224, 3) 0         \n",
      "_________________________________________________________________\n",
      "time_distributed (TimeDistri (None, None, 2048)        0         \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, None, 16)          132160    \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 8)                 800       \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 8)                 72        \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 5)                 45        \n",
      "=================================================================\n",
      "Total params: 133,077\n",
      "Trainable params: 133,077\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "837944b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_processor(labels, labels_class):\n",
    "    new_labels = np.zeros(labels.shape)\n",
    "    for i in range(labels.shape[0]):\n",
    "        index = labels_class.index(labels[i])\n",
    "        new_labels[i] = index\n",
    "        \n",
    "    return new_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a2c33fde",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_all_videos(df, root_dir):\n",
    "    num_samples = len(df)\n",
    "    video_paths = df[\"video_name\"].values.tolist()\n",
    "    labels = df[\"tag\"].values\n",
    "    labels = label_processor(labels, LABELS_CLASS)\n",
    "    labels = keras.utils.to_categorical(labels, NUM_CLASSES)\n",
    "    \n",
    "    video_batch = np.zeros((num_samples, MAX_SEQ_LENGTH, 224, 224, 3), dtype=\"float32\")\n",
    "\n",
    "    # For each video.\n",
    "    \n",
    "    for idx, path in enumerate(video_paths):\n",
    "        # Gather all its frames and add a batch dimension.\n",
    "        frames = load_video(os.path.join(root_dir, path))\n",
    "        frames = frames[None, ...]\n",
    "        \n",
    "        # Extract features from the frames of the current video.\n",
    "        for i, batch in enumerate(frames):\n",
    "            video_length = batch.shape[0]\n",
    "            select_frame = np.linspace(0, video_length-1, MAX_SEQ_LENGTH,endpoint=True,retstep=True,dtype=int)[0]\n",
    "            # length = min(MAX_SEQ_LENGTH, video_length)\n",
    "            \n",
    "            video_batch[idx] = batch[select_frame].astype('float32') / 255\n",
    "\n",
    "    return video_batch, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1d90759f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frame features in train set: (20, 224, 224, 3)\n"
     ]
    }
   ],
   "source": [
    "train_data, train_labels = prepare_all_videos(train_df, \"C:/nyu/DRL/final_project/dataset/UCF101/train\")\n",
    "print(f\"Frame features in train set: {train_data[0].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "95be77cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_data, test_labels = prepare_all_videos(test_df, \"C:/nyu/DRL/final_project/dataset/UCF101/test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9f76ee8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    loss=\"categorical_crossentropy\", optimizer=tf.keras.optimizers.Adam(), metrics=[\"accuracy\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1839653d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\irohc\\anaconda3\\envs\\drl_proj\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/20\n",
      "594/594 [==============================] - 34s 58ms/sample - loss: 1.5525 - acc: 0.3418\n",
      "Epoch 2/20\n",
      "594/594 [==============================] - 23s 39ms/sample - loss: 1.2928 - acc: 0.4865\n",
      "Epoch 3/20\n",
      "594/594 [==============================] - 23s 39ms/sample - loss: 1.1139 - acc: 0.5168\n",
      "Epoch 4/20\n",
      "594/594 [==============================] - 24s 40ms/sample - loss: 0.8878 - acc: 0.6296\n",
      "Epoch 5/20\n",
      "594/594 [==============================] - 26s 43ms/sample - loss: 0.7725 - acc: 0.6902\n",
      "Epoch 6/20\n",
      "594/594 [==============================] - 26s 44ms/sample - loss: 0.8277 - acc: 0.6751\n",
      "Epoch 7/20\n",
      "594/594 [==============================] - 27s 45ms/sample - loss: 0.6159 - acc: 0.7306\n",
      "Epoch 8/20\n",
      "594/594 [==============================] - 27s 45ms/sample - loss: 0.7043 - acc: 0.6936\n",
      "Epoch 9/20\n",
      "594/594 [==============================] - 27s 45ms/sample - loss: 0.5956 - acc: 0.7508\n",
      "Epoch 10/20\n",
      "594/594 [==============================] - 27s 45ms/sample - loss: 0.6152 - acc: 0.7323\n",
      "Epoch 11/20\n",
      "594/594 [==============================] - 26s 44ms/sample - loss: 0.5424 - acc: 0.7542\n",
      "Epoch 12/20\n",
      "594/594 [==============================] - 27s 45ms/sample - loss: 0.5247 - acc: 0.7391\n",
      "Epoch 13/20\n",
      "594/594 [==============================] - 27s 45ms/sample - loss: 0.5593 - acc: 0.7256\n",
      "Epoch 14/20\n",
      "594/594 [==============================] - 27s 45ms/sample - loss: 0.5174 - acc: 0.7508\n",
      "Epoch 15/20\n",
      "594/594 [==============================] - 27s 46ms/sample - loss: 0.4751 - acc: 0.7694\n",
      "Epoch 16/20\n",
      "594/594 [==============================] - 27s 45ms/sample - loss: 0.4779 - acc: 0.8013\n",
      "Epoch 17/20\n",
      "594/594 [==============================] - 27s 46ms/sample - loss: 0.4208 - acc: 0.8047\n",
      "Epoch 18/20\n",
      "594/594 [==============================] - 28s 47ms/sample - loss: 0.4295 - acc: 0.7710\n",
      "Epoch 19/20\n",
      "594/594 [==============================] - 28s 47ms/sample - loss: 0.4669 - acc: 0.7542\n",
      "Epoch 20/20\n",
      "594/594 [==============================] - 27s 45ms/sample - loss: 0.5191 - acc: 0.7710\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1fa2c583e48>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_data, train_labels, shuffle=True,\n",
    "      batch_size=10, epochs=20,\n",
    "      verbose=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drl_proj",
   "language": "python",
   "name": "drl_proj"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
