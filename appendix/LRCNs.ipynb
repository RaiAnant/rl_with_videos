{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "431475e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\irohc\\anaconda3\\envs\\drl_proj\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\irohc\\anaconda3\\envs\\drl_proj\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\irohc\\anaconda3\\envs\\drl_proj\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\irohc\\anaconda3\\envs\\drl_proj\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\irohc\\anaconda3\\envs\\drl_proj\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\irohc\\anaconda3\\envs\\drl_proj\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "from tensorflow_docs.vis import embed\n",
    "from tensorflow import keras\n",
    "from imutils import paths\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import imageio\n",
    "import cv2\n",
    "import os\n",
    "\n",
    "from rl_with_videos.preprocessors.convnet import convnet_preprocessor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dec51d35",
   "metadata": {},
   "source": [
    "Use UCF101 dataset as examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8cc55fff",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SIZE = 224\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 10\n",
    "\n",
    "MAX_SEQ_LENGTH = 20\n",
    "NUM_FEATURES = 2048\n",
    "\n",
    "TRAINING_FILE = \"C:/nyu/DRL/final_project/dataset/UCF101/train.csv\"\n",
    "TESTING_FILE = \"C:/nyu/DRL/final_project/dataset/UCF101/test.csv\"\n",
    "\n",
    "LABELS_CLASS = ['CricketShot', 'PlayingCello', 'Punch', 'ShavingBeard', 'TennisSwing']\n",
    "NUM_CLASSES = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a276ac23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total videos for training: 594\n",
      "Total videos for testing: 224\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>video_name</th>\n",
       "      <th>tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>295</th>\n",
       "      <td>v_Punch_g16_c04.avi</td>\n",
       "      <td>Punch</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180</th>\n",
       "      <td>v_PlayingCello_g17_c04.avi</td>\n",
       "      <td>PlayingCello</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>383</th>\n",
       "      <td>v_ShavingBeard_g11_c04.avi</td>\n",
       "      <td>ShavingBeard</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203</th>\n",
       "      <td>v_PlayingCello_g20_c06.avi</td>\n",
       "      <td>PlayingCello</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>441</th>\n",
       "      <td>v_ShavingBeard_g20_c06.avi</td>\n",
       "      <td>ShavingBeard</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>573</th>\n",
       "      <td>v_TennisSwing_g22_c04.avi</td>\n",
       "      <td>TennisSwing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>v_CricketShot_g08_c05.avi</td>\n",
       "      <td>CricketShot</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>221</th>\n",
       "      <td>v_PlayingCello_g23_c05.avi</td>\n",
       "      <td>PlayingCello</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>235</th>\n",
       "      <td>v_PlayingCello_g25_c05.avi</td>\n",
       "      <td>PlayingCello</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>v_CricketShot_g20_c07.avi</td>\n",
       "      <td>CricketShot</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     video_name           tag\n",
       "295         v_Punch_g16_c04.avi         Punch\n",
       "180  v_PlayingCello_g17_c04.avi  PlayingCello\n",
       "383  v_ShavingBeard_g11_c04.avi  ShavingBeard\n",
       "203  v_PlayingCello_g20_c06.avi  PlayingCello\n",
       "441  v_ShavingBeard_g20_c06.avi  ShavingBeard\n",
       "573   v_TennisSwing_g22_c04.avi   TennisSwing\n",
       "4     v_CricketShot_g08_c05.avi   CricketShot\n",
       "221  v_PlayingCello_g23_c05.avi  PlayingCello\n",
       "235  v_PlayingCello_g25_c05.avi  PlayingCello\n",
       "85    v_CricketShot_g20_c07.avi   CricketShot"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = pd.read_csv(TRAINING_FILE)\n",
    "test_df = pd.read_csv(TESTING_FILE)\n",
    "\n",
    "print(f\"Total videos for training: {len(train_df)}\")\n",
    "print(f\"Total videos for testing: {len(test_df)}\")\n",
    "\n",
    "train_df.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "60dc708c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_center_square(frame):\n",
    "    y, x = frame.shape[0:2]\n",
    "    min_dim = min(y, x)\n",
    "    start_x = (x // 2) - (min_dim // 2)\n",
    "    start_y = (y // 2) - (min_dim // 2)\n",
    "    return frame[start_y : start_y + min_dim, start_x : start_x + min_dim]\n",
    "\n",
    "\n",
    "def load_video(path, max_frames=0, resize=(IMG_SIZE, IMG_SIZE)):\n",
    "    cap = cv2.VideoCapture(path)\n",
    "    frames = []\n",
    "    try:\n",
    "        while True:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            frame = crop_center_square(frame)\n",
    "            frame = cv2.resize(frame, resize)\n",
    "            frame = frame[:, :, [2, 1, 0]]\n",
    "            frames.append(frame)\n",
    "\n",
    "            if len(frames) == max_frames:\n",
    "                break\n",
    "    finally:\n",
    "        cap.release()\n",
    "    return np.array(frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d6996d60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def build_feature_extractor():\n",
    "#     feature_extractor = keras.applications.InceptionV3(\n",
    "#         weights=\"imagenet\",\n",
    "#         include_top=False,\n",
    "#         pooling=\"avg\",\n",
    "#         input_shape=(IMG_SIZE, IMG_SIZE, 3),\n",
    "#     )\n",
    "#     preprocess_input = keras.applications.inception_v3.preprocess_input\n",
    "\n",
    "#     inputs = keras.Input((IMG_SIZE, IMG_SIZE, 3))\n",
    "#     preprocessed = tf.keras.layers.Lambda(\n",
    "#         lambda x: preprocess_input(x))(inputs)\n",
    "\n",
    "#     outputs = feature_extractor(preprocessed)\n",
    "#     return keras.Model(inputs, outputs, name=\"feature_extractor\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2a15f1d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\irohc\\anaconda3\\envs\\drl_proj\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "feature_extractor = keras.applications.InceptionV3(\n",
    "        weights=\"imagenet\",\n",
    "        include_top=False,\n",
    "        pooling=\"avg\",\n",
    "        input_shape=(IMG_SIZE, IMG_SIZE, 3),\n",
    ")\n",
    "feature_extractor.trainable = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fe993d22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature_extractor = build_feature_extractor()\n",
    "# feature_extractor.trainable = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7975f696",
   "metadata": {},
   "outputs": [],
   "source": [
    "video = keras.layers.Input(shape=(None,224,224,3),name='video_input')\n",
    "encoded_frame = keras.layers.TimeDistributed(keras.layers.Lambda(lambda x: feature_extractor(x)))(video)\n",
    "encoded_vid = keras.layers.LSTM(16, return_sequences=True)(encoded_frame)\n",
    "encoded_vid = keras.layers.LSTM(8, return_sequences=False)(encoded_vid)\n",
    "encoded_vid = keras.layers.Dense(8, activation='relu')(encoded_vid)\n",
    "outputs = keras.layers.Dense(5, activation='softmax')(encoded_vid)\n",
    "model = keras.models.Model(inputs=[video],outputs=outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "86ceb059",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "video_input (InputLayer)     (None, None, 224, 224, 3) 0         \n",
      "_________________________________________________________________\n",
      "time_distributed (TimeDistri (None, None, 2048)        0         \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, None, 16)          132160    \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 8)                 800       \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 8)                 72        \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 5)                 45        \n",
      "=================================================================\n",
      "Total params: 133,077\n",
      "Trainable params: 133,077\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "837944b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_processor(labels, labels_class):\n",
    "    new_labels = np.zeros(labels.shape)\n",
    "    for i in range(labels.shape[0]):\n",
    "        index = labels_class.index(labels[i])\n",
    "        new_labels[i] = index\n",
    "        \n",
    "    return new_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a2c33fde",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_all_videos(df, root_dir):\n",
    "    num_samples = len(df)\n",
    "    video_paths = df[\"video_name\"].values.tolist()\n",
    "    labels = df[\"tag\"].values\n",
    "    labels = label_processor(labels, LABELS_CLASS)\n",
    "    labels = keras.utils.to_categorical(labels, NUM_CLASSES)\n",
    "    \n",
    "    video_batch = np.zeros((num_samples, MAX_SEQ_LENGTH, 224, 224, 3), dtype=\"float32\")\n",
    "\n",
    "    # For each video.\n",
    "    \n",
    "    for idx, path in enumerate(video_paths):\n",
    "        # Gather all its frames and add a batch dimension.\n",
    "        frames = load_video(os.path.join(root_dir, path))\n",
    "        frames = frames[None, ...]\n",
    "        \n",
    "        # Extract features from the frames of the current video.\n",
    "        for i, batch in enumerate(frames):\n",
    "            video_length = batch.shape[0]\n",
    "            select_frame = np.linspace(0, video_length-1, MAX_SEQ_LENGTH,endpoint=True,retstep=True,dtype=int)[0]\n",
    "            # length = min(MAX_SEQ_LENGTH, video_length)\n",
    "            \n",
    "            video_batch[idx] = batch[select_frame].astype('float32') / 255\n",
    "\n",
    "    return video_batch, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1d90759f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frame features in train set: (20, 224, 224, 3)\n"
     ]
    }
   ],
   "source": [
    "train_data, train_labels = prepare_all_videos(train_df, \"C:/nyu/DRL/final_project/dataset/UCF101/train\")\n",
    "print(f\"Frame features in train set: {train_data[0].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "95be77cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_data, test_labels = prepare_all_videos(test_df, \"C:/nyu/DRL/final_project/dataset/UCF101/test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9f76ee8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    loss=\"categorical_crossentropy\", optimizer=tf.keras.optimizers.Adam(), metrics=[\"accuracy\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1839653d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 504 samples, validate on 90 samples\n",
      "WARNING:tensorflow:From C:\\Users\\irohc\\anaconda3\\envs\\drl_proj\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/20\n",
      "504/504 [==============================] - 44s 87ms/sample - loss: 1.5107 - acc: 0.2996 - val_loss: 2.0638 - val_acc: 0.0000e+00\n",
      "Epoch 2/20\n",
      "504/504 [==============================] - 34s 67ms/sample - loss: 1.2815 - acc: 0.6091 - val_loss: 1.9609 - val_acc: 0.0000e+00\n",
      "Epoch 3/20\n",
      "504/504 [==============================] - 25s 49ms/sample - loss: 1.0674 - acc: 0.6349 - val_loss: 2.3127 - val_acc: 0.0000e+00\n",
      "Epoch 4/20\n",
      "504/504 [==============================] - 26s 51ms/sample - loss: 0.9534 - acc: 0.6270 - val_loss: 2.6647 - val_acc: 0.0000e+00\n",
      "Epoch 5/20\n",
      "504/504 [==============================] - 26s 52ms/sample - loss: 1.0166 - acc: 0.6032 - val_loss: 2.3821 - val_acc: 0.0000e+00\n",
      "Epoch 6/20\n",
      "504/504 [==============================] - 28s 55ms/sample - loss: 0.7583 - acc: 0.7202 - val_loss: 2.9964 - val_acc: 0.0000e+00\n",
      "Epoch 7/20\n",
      "504/504 [==============================] - 27s 54ms/sample - loss: 0.8758 - acc: 0.6171 - val_loss: 2.7084 - val_acc: 0.0000e+00\n",
      "Epoch 8/20\n",
      "300/504 [================>.............] - ETA: 9s - loss: 0.6959 - acc: 0.7200"
     ]
    }
   ],
   "source": [
    "model.fit(train_data, train_labels, shuffle=True,\n",
    "      batch_size=10, epochs=20, validation_split=0.15,\n",
    "      verbose=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drl_proj",
   "language": "python",
   "name": "drl_proj"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
