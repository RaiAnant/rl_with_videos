{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b6f89972",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\irohc\\anaconda3\\envs\\drl_proj\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\irohc\\anaconda3\\envs\\drl_proj\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\irohc\\anaconda3\\envs\\drl_proj\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\irohc\\anaconda3\\envs\\drl_proj\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\irohc\\anaconda3\\envs\\drl_proj\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\irohc\\anaconda3\\envs\\drl_proj\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "from tensorflow_docs.vis import embed\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import backend as K\n",
    "from imutils import paths\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import imageio\n",
    "import cv2\n",
    "import os\n",
    "\n",
    "from rl_with_videos.preprocessors.convnet import convnet_preprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "be662734",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SIZE = 48\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 10\n",
    "\n",
    "MAX_SEQ_LENGTH = 20\n",
    "NUM_FEATURES = 2048\n",
    "\n",
    "TRAINING_FILE = \"C:/nyu/DRL/final_project/dataset/UCF101/train.csv\"\n",
    "TESTING_FILE = \"C:/nyu/DRL/final_project/dataset/UCF101/test.csv\"\n",
    "\n",
    "LABELS_CLASS = ['CricketShot', 'PlayingCello', 'Punch', 'ShavingBeard', 'TennisSwing']\n",
    "NUM_CLASSES = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7361eed8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total videos for training: 594\n",
      "Total videos for testing: 224\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>video_name</th>\n",
       "      <th>tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>277</th>\n",
       "      <td>v_Punch_g13_c06.avi</td>\n",
       "      <td>Punch</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>v_CricketShot_g10_c04.avi</td>\n",
       "      <td>CricketShot</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>v_CricketShot_g24_c02.avi</td>\n",
       "      <td>CricketShot</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>v_CricketShot_g09_c04.avi</td>\n",
       "      <td>CricketShot</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>v_CricketShot_g15_c03.avi</td>\n",
       "      <td>CricketShot</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>532</th>\n",
       "      <td>v_TennisSwing_g16_c01.avi</td>\n",
       "      <td>TennisSwing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>503</th>\n",
       "      <td>v_TennisSwing_g11_c07.avi</td>\n",
       "      <td>TennisSwing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>v_CricketShot_g17_c02.avi</td>\n",
       "      <td>CricketShot</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>462</th>\n",
       "      <td>v_ShavingBeard_g23_c06.avi</td>\n",
       "      <td>ShavingBeard</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>181</th>\n",
       "      <td>v_PlayingCello_g17_c05.avi</td>\n",
       "      <td>PlayingCello</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     video_name           tag\n",
       "277         v_Punch_g13_c06.avi         Punch\n",
       "17    v_CricketShot_g10_c04.avi   CricketShot\n",
       "105   v_CricketShot_g24_c02.avi   CricketShot\n",
       "10    v_CricketShot_g09_c04.avi   CricketShot\n",
       "51    v_CricketShot_g15_c03.avi   CricketShot\n",
       "532   v_TennisSwing_g16_c01.avi   TennisSwing\n",
       "503   v_TennisSwing_g11_c07.avi   TennisSwing\n",
       "64    v_CricketShot_g17_c02.avi   CricketShot\n",
       "462  v_ShavingBeard_g23_c06.avi  ShavingBeard\n",
       "181  v_PlayingCello_g17_c05.avi  PlayingCello"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = pd.read_csv(TRAINING_FILE)\n",
    "test_df = pd.read_csv(TESTING_FILE)\n",
    "\n",
    "print(f\"Total videos for training: {len(train_df)}\")\n",
    "print(f\"Total videos for testing: {len(test_df)}\")\n",
    "\n",
    "train_df.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1d9d6d42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_center_square(frame):\n",
    "    y, x = frame.shape[0:2]\n",
    "    min_dim = min(y, x)\n",
    "    start_x = (x // 2) - (min_dim // 2)\n",
    "    start_y = (y // 2) - (min_dim // 2)\n",
    "    return frame[start_y : start_y + min_dim, start_x : start_x + min_dim]\n",
    "\n",
    "\n",
    "def load_video(path, max_frames=0, resize=(IMG_SIZE, IMG_SIZE)):\n",
    "    cap = cv2.VideoCapture(path)\n",
    "    frames = []\n",
    "    try:\n",
    "        while True:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            frame = crop_center_square(frame)\n",
    "            frame = cv2.resize(frame, resize)\n",
    "            frame = frame[:, :, [2, 1, 0]]\n",
    "            frames.append(frame)\n",
    "\n",
    "            if len(frames) == max_frames:\n",
    "                break\n",
    "    finally:\n",
    "        cap.release()\n",
    "    return np.array(frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1686970d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kwargs: {}\n",
      "WARNING:tensorflow:From C:\\Users\\irohc\\anaconda3\\envs\\drl_proj\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "kwargs: {}\n",
      "name: feedforward_model\n",
      "inputs: [<tf.Tensor 'input_2:0' shape=(?, 4608) dtype=float32>]\n"
     ]
    }
   ],
   "source": [
    "feature_extractor = convnet_preprocessor([(6912,)], (48,48,3), 256)\n",
    "feature_extractor.trainable = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "74d155a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 6912)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lambda (Lambda)                 (None, 6912)         0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_1 (Lambda)               [(None, 6912), (None 0           lambda[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "reshape (Reshape)               (None, 48, 48, 3)    0           lambda_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d (Conv2D)                 (None, 48, 48, 32)   2432        reshape[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D)    (None, 24, 24, 32)   0           conv2d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 24, 24, 32)   25632       max_pooling2d[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2D)  (None, 12, 12, 32)   0           conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 4608)         0           max_pooling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "lambda_2 (Lambda)               (None, 4608)         0           flatten[0][0]                    \n",
      "                                                                 lambda_1[0][1]                   \n",
      "__________________________________________________________________________________________________\n",
      "feedforward_model (PicklableKer (None, 256)          315776      lambda_2[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 343,840\n",
      "Trainable params: 343,840\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "feature_extractor.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "75bd5cac",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEmbedding(keras.layers.Layer):\n",
    "    def __init__(self, sequence_length, output_dim, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.position_embeddings = keras.layers.Embedding(\n",
    "            input_dim=sequence_length, output_dim=output_dim\n",
    "        )\n",
    "        self.sequence_length = sequence_length\n",
    "        self.output_dim = output_dim\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # The inputs are of shape: `(batch_size, frames, num_features)`\n",
    "        length = tf.shape(inputs)[1]\n",
    "        positions = tf.range(start=0, limit=length, delta=1)\n",
    "        embedded_positions = self.position_embeddings(positions)\n",
    "        return inputs + embedded_positions\n",
    "\n",
    "#     def compute_mask(self, inputs, mask=None):\n",
    "#         mask = tf.reduce_any(tf.cast(inputs, \"bool\"), axis=-1)\n",
    "#         return mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9317b1dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(keras.layers.Layer):\n",
    "    \n",
    "    def __init__(self,multiheads, head_dim,mask_right=False,**kwargs):\n",
    "        self.multiheads = multiheads\n",
    "        self.head_dim = head_dim\n",
    "        self.output_dim = multiheads * head_dim\n",
    "        self.mask_right = mask_right\n",
    "        super(MultiHeadAttention, self).__init__(**kwargs)\n",
    "        \n",
    "    def compute_output_shape(self,input_shape):\n",
    "        return (input_shape[0][0],input_shape[0][1],self.output_dim) #shape=[batch_size,Q_sequence_length,self.multiheads*self.head_dim]\n",
    "\n",
    "    def build(self,input_shape):\n",
    "        self.WQ = self.add_weight(name='WQ', \n",
    "                                  shape=(input_shape[0][-1].value, self.output_dim),#input_shape[0] -> Q_seq\n",
    "                                  initializer='glorot_uniform',\n",
    "                                  trainable=True)\n",
    "        self.WK = self.add_weight(name='WK', \n",
    "                                  shape=(input_shape[1][-1].value, self.output_dim),#input_shape[1] -> K_seq\n",
    "                                  initializer='glorot_uniform',\n",
    "                                  trainable=True)\n",
    "        self.WV = self.add_weight(name='WV', \n",
    "                                  shape=(input_shape[2][-1].value, self.output_dim),#input_shape[2] -> V_seq\n",
    "                                  initializer='glorot_uniform',\n",
    "                                  trainable=True)\n",
    "        super(MultiHeadAttention, self).build(input_shape)\n",
    "    \n",
    "    def Mask(self,inputs,seq_len,mode='add'):\n",
    "        if seq_len == None:\n",
    "            return inputs\n",
    "        else:\n",
    "            mask = K.one_hot(indices=seq_len[:,0],num_classes=K.shape(inputs)[1])#mask.shape=[batch_size,short_sequence_length],mask=[[0,0,0,0,1,0,0,..],[0,1,0,0,0,0,0...]...]\n",
    "            mask = 1 - K.cumsum(mask,axis=1)#mask.shape=[batch_size,short_sequence_length],mask=[[1,1,1,1,0,0,0,...],[1,0,0,0,0,0,0,...]...]\n",
    "            for _ in range(len(inputs.shape)-2):\n",
    "                mask = K.expand_dims(mask, 2)\n",
    "            if mode == 'mul':\n",
    "                return inputs * mask\n",
    "            elif mode == 'add':\n",
    "                return inputs - (1 - mask) * 1e12\n",
    "    \n",
    "    def call(self,QKVs):\n",
    "\n",
    "        if len(QKVs) == 3:\n",
    "            Q_seq,K_seq,V_seq = QKVs\n",
    "            Q_len,V_len = None,None\n",
    "        elif len(QKVs) == 5:\n",
    "            Q_seq,K_seq,V_seq,Q_len,V_len = QKVs\n",
    " \n",
    "        Q_seq = K.dot(Q_seq,self.WQ)#Q_seq.shape=[batch_size,Q_sequence_length,self.output_dim]=[batch_size,Q_sequence_length,self.multiheads*self.head_dim] \n",
    "        Q_seq = K.reshape(Q_seq,shape=(-1,K.shape(Q_seq)[1],self.multiheads,self.head_dim))#Q_seq.shape=[batch_size,Q_sequence_length,self.multiheads,self.head_dim]\n",
    "        Q_seq = K.permute_dimensions(Q_seq,pattern=(0,2,1,3))#Q_seq.shape=[batch_size,self.multiheads,Q_sequence_length,self.head_dim]\n",
    "\n",
    "        K_seq = K.dot(K_seq,self.WK)\n",
    "        K_seq = K.reshape(K_seq,shape=(-1,K.shape(K_seq)[1],self.multiheads,self.head_dim))\n",
    "        K_seq = K.permute_dimensions(K_seq,pattern=(0,2,1,3))\n",
    "\n",
    "        V_seq = K.dot(V_seq,self.WV)\n",
    "        V_seq = K.reshape(V_seq,shape=(-1,K.shape(V_seq)[1],self.multiheads,self.head_dim))\n",
    "        V_seq = K.permute_dimensions(V_seq,pattern=(0,2,1,3))\n",
    "\n",
    "        A = K.batch_dot(Q_seq,K_seq,axes=[3,3])/K.sqrt(K.cast(self.head_dim,dtype='float32'))#A.shape=[batch_size,self.multiheads,Q_sequence_length,K_sequence_length]\n",
    "        A = K.permute_dimensions(A,pattern=(0,3,2,1))#A.shape=[batch_size,K_sequence_length,Q_sequence_length,self.multiheads]\n",
    "\n",
    "        A = self.Mask(A,V_len,'add')\n",
    "        A = K.permute_dimensions(A,pattern=(0,3,2,1))#A.shape=[batch_size,self.multiheads,Q_sequence_length,K_sequence_length]\n",
    "        \n",
    "        if self.mask_right:\n",
    "            ones = K.ones_like(A[:1,:1])\n",
    "            lower_triangular = K.tf.matrix_band_part(ones,num_lower=-1,num_upper=0) \n",
    "            mask = (ones - lower_triangular) * 1e12 \n",
    "            A = A - mask #Element-wise subtractï¼ŒA.shape=[batch_size,self.multiheads,Q_sequence_length,K_sequence_length]\n",
    "        A = K.softmax(A) #A.shape=[batch_size,self.multiheads,Q_sequence_length,K_sequence_length]\n",
    "        #V_seq.shape=[batch_size,V_sequence_length,V_embedding_dim]\n",
    "        O_seq = K.batch_dot(A,V_seq,axes=[3,2])#O_seq.shape=[batch_size,self.multiheads,Q_sequence_length,V_sequence_length]\n",
    "        O_seq = K.permute_dimensions(O_seq,pattern=(0,2,1,3))#O_seq.shape=[batch_size,Q_sequence_length,self.multiheads,V_sequence_length]\n",
    "        O_seq = K.reshape(O_seq,shape=(-1,K.shape(O_seq)[1],self.output_dim))#O_seq.shape=[,Q_sequence_length,self.multiheads*self.head_dim]\n",
    "        O_seq = self.Mask(O_seq,Q_len,'mul')\n",
    "        return O_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "70fbd97e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoder(keras.layers.Layer):\n",
    "    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.embed_dim = embed_dim\n",
    "        self.dense_dim = dense_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.attention = MultiHeadAttention(\n",
    "            num_heads, embed_dim\n",
    "        )\n",
    "        self.dense_proj = keras.Sequential(\n",
    "            [keras.layers.Dense(dense_dim, activation='relu'), keras.layers.Dense(embed_dim),]\n",
    "        )\n",
    "#         self.layernorm_1 = keras.layers.LayerNormalization()\n",
    "#         self.layernorm_2 = keras.layers.LayerNormalization()\n",
    "\n",
    "    def call(self, inputs, mask=None):\n",
    "        if mask is not None:\n",
    "            mask = mask[:, tf.newaxis, :]\n",
    "\n",
    "        attention_output = self.attention([inputs, inputs, inputs])\n",
    "        proj_input = keras.layers.BatchNormalization()(inputs + attention_output)\n",
    "        proj_output = self.dense_proj(proj_input)\n",
    "        return keras.layers.BatchNormalization()(proj_input + proj_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a48fb939",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Transformer(\n",
    "        input_shapes,\n",
    "        output_size,\n",
    "        feature_extractor,\n",
    "        hidden_state_num = 2,\n",
    "        hidden_state_size = (16, 8),\n",
    "        *args,\n",
    "        **kwargs):\n",
    "    sequence_length = MAX_SEQ_LENGTH\n",
    "    embed_dim = 256\n",
    "    dense_dim = 4\n",
    "    num_heads = 1\n",
    "    video = keras.layers.Input(shape=input_shapes,name='video_input')\n",
    "    encoded_frame = keras.layers.TimeDistributed(keras.layers.Lambda(lambda x: feature_extractor(x)))(video)\n",
    "    \n",
    "    x = PositionalEmbedding(\n",
    "        sequence_length, embed_dim, name=\"frame_position_embedding\"\n",
    "    )(encoded_frame)\n",
    "    x = TransformerEncoder(embed_dim, dense_dim, num_heads, name=\"transformer_layer\")(x)\n",
    "    x = keras.layers.GlobalMaxPooling1D()(x)\n",
    "    x = keras.layers.Dropout(0.5)(x)\n",
    "        \n",
    "    # encoded_vid = keras.layers.Dense(8, activation='relu')(encoded_vid)\n",
    "    outputs = keras.layers.Dense(output_size, activation='softmax')(x)\n",
    "    \n",
    "    model = keras.models.Model(inputs=[video],outputs=outputs)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0ab182dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\irohc\\anaconda3\\envs\\drl_proj\\lib\\site-packages\\tensorflow\\python\\keras\\layers\\core.py:143: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    }
   ],
   "source": [
    "model = Transformer((None, 6912), 5, feature_extractor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e6156671",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "video_input (InputLayer)     (None, None, 6912)        0         \n",
      "_________________________________________________________________\n",
      "time_distributed (TimeDistri (None, None, 256)         0         \n",
      "_________________________________________________________________\n",
      "frame_position_embedding (Po (None, None, 256)         5120      \n",
      "_________________________________________________________________\n",
      "transformer_layer (Transform (None, None, 256)         198916    \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d (Global (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 5)                 1285      \n",
      "=================================================================\n",
      "Total params: 205,321\n",
      "Trainable params: 205,321\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "671d8df0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_processor(labels, labels_class):\n",
    "    new_labels = np.zeros(labels.shape)\n",
    "    for i in range(labels.shape[0]):\n",
    "        index = labels_class.index(labels[i])\n",
    "        new_labels[i] = index\n",
    "        \n",
    "    return new_labels\n",
    "\n",
    "def prepare_all_videos(df, root_dir):\n",
    "    num_samples = len(df)\n",
    "    video_paths = df[\"video_name\"].values.tolist()\n",
    "    labels = df[\"tag\"].values\n",
    "    labels = label_processor(labels, LABELS_CLASS)\n",
    "    labels = keras.utils.to_categorical(labels, NUM_CLASSES)\n",
    "    \n",
    "    video_batch = np.zeros((num_samples, MAX_SEQ_LENGTH, 6912), dtype=\"float32\")\n",
    "\n",
    "    # For each video.\n",
    "    \n",
    "    for idx, path in enumerate(video_paths):\n",
    "        # Gather all its frames and add a batch dimension.\n",
    "        frames = load_video(os.path.join(root_dir, path))\n",
    "        frames = frames[None, ...]\n",
    "        \n",
    "        # Extract features from the frames of the current video.\n",
    "        for i, batch in enumerate(frames):\n",
    "            video_length = batch.shape[0]\n",
    "            select_frame = np.linspace(0, video_length-1, MAX_SEQ_LENGTH,endpoint=True,retstep=True,dtype=int)[0]\n",
    "            # length = min(MAX_SEQ_LENGTH, video_length)\n",
    "            \n",
    "            video_batch[idx] = batch[select_frame].reshape(20, 6912).astype('float32') / 255\n",
    "\n",
    "    return video_batch, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "79832568",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frame features in train set: (20, 6912)\n"
     ]
    }
   ],
   "source": [
    "train_data, train_labels = prepare_all_videos(train_df, \"C:/nyu/DRL/final_project/dataset/UCF101/train\")\n",
    "print(f\"Frame features in train set: {train_data[0].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c3a87197",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    loss=\"categorical_crossentropy\", optimizer=tf.keras.optimizers.Adam(), metrics=[\"accuracy\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fec16533",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 534 samples, validate on 60 samples\n",
      "WARNING:tensorflow:From C:\\Users\\irohc\\anaconda3\\envs\\drl_proj\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "WARNING:tensorflow:From C:\\Users\\irohc\\anaconda3\\envs\\drl_proj\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py:102: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n",
      "Epoch 1/30\n",
      "534/534 [==============================] - 4s 8ms/sample - loss: 2.5349 - acc: 0.3258 - val_loss: 1.4886 - val_acc: 0.2833\n",
      "Epoch 2/30\n",
      "534/534 [==============================] - 0s 822us/sample - loss: 1.7134 - acc: 0.4588 - val_loss: 1.5158 - val_acc: 0.0667\n",
      "Epoch 3/30\n",
      "534/534 [==============================] - 0s 769us/sample - loss: 1.2556 - acc: 0.6311 - val_loss: 1.6122 - val_acc: 0.0500\n",
      "Epoch 4/30\n",
      "534/534 [==============================] - 0s 741us/sample - loss: 1.0698 - acc: 0.6404 - val_loss: 1.6078 - val_acc: 0.0667\n",
      "Epoch 5/30\n",
      "534/534 [==============================] - 0s 728us/sample - loss: 1.0085 - acc: 0.6554 - val_loss: 1.5806 - val_acc: 0.0667\n",
      "Epoch 6/30\n",
      "534/534 [==============================] - 0s 728us/sample - loss: 0.9401 - acc: 0.6816 - val_loss: 1.5896 - val_acc: 0.0500\n",
      "Epoch 7/30\n",
      "534/534 [==============================] - 0s 727us/sample - loss: 0.8622 - acc: 0.7210 - val_loss: 1.6374 - val_acc: 0.0500\n",
      "Epoch 8/30\n",
      "534/534 [==============================] - 0s 732us/sample - loss: 0.8108 - acc: 0.7322 - val_loss: 1.6353 - val_acc: 0.0500\n",
      "Epoch 9/30\n",
      "534/534 [==============================] - 0s 732us/sample - loss: 0.7743 - acc: 0.7022 - val_loss: 1.5691 - val_acc: 0.0833\n",
      "Epoch 10/30\n",
      "534/534 [==============================] - 0s 727us/sample - loss: 0.7115 - acc: 0.7603 - val_loss: 1.6449 - val_acc: 0.0500\n",
      "Epoch 11/30\n",
      "534/534 [==============================] - 0s 749us/sample - loss: 0.6358 - acc: 0.7697 - val_loss: 1.6649 - val_acc: 0.0500\n",
      "Epoch 12/30\n",
      "534/534 [==============================] - 0s 759us/sample - loss: 0.5181 - acc: 0.8052 - val_loss: 1.6367 - val_acc: 0.0500\n",
      "Epoch 13/30\n",
      "534/534 [==============================] - 0s 745us/sample - loss: 0.5454 - acc: 0.8034 - val_loss: 1.6556 - val_acc: 0.0500\n",
      "Epoch 14/30\n",
      "534/534 [==============================] - 0s 734us/sample - loss: 0.4355 - acc: 0.8483 - val_loss: 1.6151 - val_acc: 0.0833\n",
      "Epoch 15/30\n",
      "534/534 [==============================] - 0s 732us/sample - loss: 0.4870 - acc: 0.8015 - val_loss: 1.6923 - val_acc: 0.0333\n",
      "Epoch 16/30\n",
      "534/534 [==============================] - 0s 723us/sample - loss: 0.4811 - acc: 0.8296 - val_loss: 1.7160 - val_acc: 0.0167\n",
      "Epoch 17/30\n",
      "534/534 [==============================] - 0s 770us/sample - loss: 0.4227 - acc: 0.8502 - val_loss: 1.6977 - val_acc: 0.0167\n",
      "Epoch 18/30\n",
      "534/534 [==============================] - 0s 726us/sample - loss: 0.3732 - acc: 0.8558 - val_loss: 1.7039 - val_acc: 0.0167\n",
      "Epoch 19/30\n",
      "534/534 [==============================] - 0s 715us/sample - loss: 0.3258 - acc: 0.8745 - val_loss: 1.7215 - val_acc: 0.0167\n",
      "Epoch 20/30\n",
      "534/534 [==============================] - 0s 723us/sample - loss: 0.3378 - acc: 0.8764 - val_loss: 1.7251 - val_acc: 0.0167\n",
      "Epoch 21/30\n",
      "534/534 [==============================] - 0s 719us/sample - loss: 0.3125 - acc: 0.8839 - val_loss: 1.7390 - val_acc: 0.0167\n",
      "Epoch 22/30\n",
      "534/534 [==============================] - 0s 709us/sample - loss: 0.3142 - acc: 0.8801 - val_loss: 1.6937 - val_acc: 0.0167\n",
      "Epoch 23/30\n",
      "534/534 [==============================] - 0s 715us/sample - loss: 0.3217 - acc: 0.8820 - val_loss: 1.7460 - val_acc: 0.0167\n",
      "Epoch 24/30\n",
      "534/534 [==============================] - 0s 704us/sample - loss: 0.2806 - acc: 0.8914 - val_loss: 1.7468 - val_acc: 0.0167\n",
      "Epoch 25/30\n",
      "534/534 [==============================] - 0s 710us/sample - loss: 0.2430 - acc: 0.9157 - val_loss: 1.7621 - val_acc: 0.0167\n",
      "Epoch 26/30\n",
      "534/534 [==============================] - 0s 719us/sample - loss: 0.2231 - acc: 0.9232 - val_loss: 1.7746 - val_acc: 0.0000e+00\n",
      "Epoch 27/30\n",
      "534/534 [==============================] - 0s 762us/sample - loss: 0.2119 - acc: 0.9176 - val_loss: 1.7349 - val_acc: 0.0167\n",
      "Epoch 28/30\n",
      "534/534 [==============================] - 0s 719us/sample - loss: 0.2110 - acc: 0.9101 - val_loss: 1.8588 - val_acc: 0.0000e+00\n",
      "Epoch 29/30\n",
      "534/534 [==============================] - 0s 730us/sample - loss: 0.1875 - acc: 0.9270 - val_loss: 1.8069 - val_acc: 0.0167\n",
      "Epoch 30/30\n",
      "534/534 [==============================] - 0s 722us/sample - loss: 0.2209 - acc: 0.9082 - val_loss: 1.7540 - val_acc: 0.0000e+00\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1d63f691308>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_data, train_labels, shuffle=True,\n",
    "      batch_size=50, epochs=30, validation_split=0.1,\n",
    "      verbose=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drl_proj",
   "language": "python",
   "name": "drl_proj"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
