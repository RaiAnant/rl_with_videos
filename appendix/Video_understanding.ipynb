{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b6f89972",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\irohc\\anaconda3\\envs\\drl_proj\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\irohc\\anaconda3\\envs\\drl_proj\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\irohc\\anaconda3\\envs\\drl_proj\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\irohc\\anaconda3\\envs\\drl_proj\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\irohc\\anaconda3\\envs\\drl_proj\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\irohc\\anaconda3\\envs\\drl_proj\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "from tensorflow_docs.vis import embed\n",
    "from tensorflow import keras\n",
    "from imutils import paths\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import imageio\n",
    "import cv2\n",
    "import os\n",
    "\n",
    "from rl_with_videos.preprocessors.convnet import convnet_preprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "be662734",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SIZE = 48\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 10\n",
    "\n",
    "MAX_SEQ_LENGTH = 20\n",
    "NUM_FEATURES = 2048\n",
    "\n",
    "TRAINING_FILE = \"C:/nyu/DRL/final_project/dataset/UCF101/train.csv\"\n",
    "TESTING_FILE = \"C:/nyu/DRL/final_project/dataset/UCF101/test.csv\"\n",
    "\n",
    "LABELS_CLASS = ['CricketShot', 'PlayingCello', 'Punch', 'ShavingBeard', 'TennisSwing']\n",
    "NUM_CLASSES = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7361eed8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total videos for training: 594\n",
      "Total videos for testing: 224\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>video_name</th>\n",
       "      <th>tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>294</th>\n",
       "      <td>v_Punch_g16_c03.avi</td>\n",
       "      <td>Punch</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>343</th>\n",
       "      <td>v_Punch_g23_c06.avi</td>\n",
       "      <td>Punch</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>275</th>\n",
       "      <td>v_Punch_g13_c04.avi</td>\n",
       "      <td>Punch</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>v_CricketShot_g20_c04.avi</td>\n",
       "      <td>CricketShot</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>257</th>\n",
       "      <td>v_Punch_g10_c06.avi</td>\n",
       "      <td>Punch</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>282</th>\n",
       "      <td>v_Punch_g14_c04.avi</td>\n",
       "      <td>Punch</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>208</th>\n",
       "      <td>v_PlayingCello_g21_c04.avi</td>\n",
       "      <td>PlayingCello</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>550</th>\n",
       "      <td>v_TennisSwing_g18_c05.avi</td>\n",
       "      <td>TennisSwing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>540</th>\n",
       "      <td>v_TennisSwing_g17_c02.avi</td>\n",
       "      <td>TennisSwing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>476</th>\n",
       "      <td>v_ShavingBeard_g25_c06.avi</td>\n",
       "      <td>ShavingBeard</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     video_name           tag\n",
       "294         v_Punch_g16_c03.avi         Punch\n",
       "343         v_Punch_g23_c06.avi         Punch\n",
       "275         v_Punch_g13_c04.avi         Punch\n",
       "82    v_CricketShot_g20_c04.avi   CricketShot\n",
       "257         v_Punch_g10_c06.avi         Punch\n",
       "282         v_Punch_g14_c04.avi         Punch\n",
       "208  v_PlayingCello_g21_c04.avi  PlayingCello\n",
       "550   v_TennisSwing_g18_c05.avi   TennisSwing\n",
       "540   v_TennisSwing_g17_c02.avi   TennisSwing\n",
       "476  v_ShavingBeard_g25_c06.avi  ShavingBeard"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = pd.read_csv(TRAINING_FILE)\n",
    "test_df = pd.read_csv(TESTING_FILE)\n",
    "\n",
    "print(f\"Total videos for training: {len(train_df)}\")\n",
    "print(f\"Total videos for testing: {len(test_df)}\")\n",
    "\n",
    "train_df.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1d9d6d42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_center_square(frame):\n",
    "    y, x = frame.shape[0:2]\n",
    "    min_dim = min(y, x)\n",
    "    start_x = (x // 2) - (min_dim // 2)\n",
    "    start_y = (y // 2) - (min_dim // 2)\n",
    "    return frame[start_y : start_y + min_dim, start_x : start_x + min_dim]\n",
    "\n",
    "\n",
    "def load_video(path, max_frames=0, resize=(IMG_SIZE, IMG_SIZE)):\n",
    "    cap = cv2.VideoCapture(path)\n",
    "    frames = []\n",
    "    try:\n",
    "        while True:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            frame = crop_center_square(frame)\n",
    "            frame = cv2.resize(frame, resize)\n",
    "            frame = frame[:, :, [2, 1, 0]]\n",
    "            frames.append(frame)\n",
    "\n",
    "            if len(frames) == max_frames:\n",
    "                break\n",
    "    finally:\n",
    "        cap.release()\n",
    "    return np.array(frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1686970d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kwargs: {}\n",
      "WARNING:tensorflow:From C:\\Users\\irohc\\anaconda3\\envs\\drl_proj\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "kwargs: {}\n",
      "name: feedforward_model\n",
      "inputs: [<tf.Tensor 'input_2:0' shape=(?, 4608) dtype=float32>]\n"
     ]
    }
   ],
   "source": [
    "feature_extractor = convnet_preprocessor([(6912,)], (48,48,3), 256)\n",
    "feature_extractor.trainable = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "74d155a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 6912)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lambda (Lambda)                 (None, 6912)         0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_1 (Lambda)               [(None, 6912), (None 0           lambda[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "reshape (Reshape)               (None, 48, 48, 3)    0           lambda_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d (Conv2D)                 (None, 48, 48, 32)   2432        reshape[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D)    (None, 24, 24, 32)   0           conv2d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 24, 24, 32)   25632       max_pooling2d[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2D)  (None, 12, 12, 32)   0           conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 4608)         0           max_pooling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "lambda_2 (Lambda)               (None, 4608)         0           flatten[0][0]                    \n",
      "                                                                 lambda_1[0][1]                   \n",
      "__________________________________________________________________________________________________\n",
      "feedforward_model (PicklableKer (None, 256)          315776      lambda_2[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 343,840\n",
      "Trainable params: 343,840\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "feature_extractor.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a48fb939",
   "metadata": {},
   "outputs": [],
   "source": [
    "def LRCNs(\n",
    "        input_shapes,\n",
    "        output_size,\n",
    "        feature_extractor,\n",
    "        hidden_state_num = 2,\n",
    "        hidden_state_size = (16, 8),\n",
    "        *args,\n",
    "        **kwargs):\n",
    "    video = keras.layers.Input(shape=input_shapes,name='video_input')\n",
    "    encoded_frame = keras.layers.TimeDistributed(keras.layers.Lambda(lambda x: feature_extractor(x)))(video)\n",
    "    \n",
    "    for i in range(0, hidden_state_num - 1):\n",
    "        encoded_frame = keras.layers.LSTM(hidden_state_size[i], return_sequences=True)(encoded_frame)\n",
    "        \n",
    "    encoded_vid = keras.layers.LSTM(hidden_state_size[hidden_state_num-1], return_sequences=False)(encoded_frame)\n",
    "        \n",
    "    # encoded_vid = keras.layers.Dense(8, activation='relu')(encoded_vid)\n",
    "    outputs = keras.layers.Dense(output_size, activation='softmax')(encoded_vid)\n",
    "    \n",
    "    model = keras.models.Model(inputs=[video],outputs=outputs)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0ab182dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LRCNs((None, 6912), 5, feature_extractor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e6156671",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "video_input (InputLayer)     (None, None, 6912)        0         \n",
      "_________________________________________________________________\n",
      "time_distributed (TimeDistri (None, None, 256)         0         \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, None, 16)          17472     \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 8)                 800       \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 5)                 45        \n",
      "=================================================================\n",
      "Total params: 18,317\n",
      "Trainable params: 18,317\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "671d8df0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_processor(labels, labels_class):\n",
    "    new_labels = np.zeros(labels.shape)\n",
    "    for i in range(labels.shape[0]):\n",
    "        index = labels_class.index(labels[i])\n",
    "        new_labels[i] = index\n",
    "        \n",
    "    return new_labels\n",
    "\n",
    "def prepare_all_videos(df, root_dir):\n",
    "    num_samples = len(df)\n",
    "    video_paths = df[\"video_name\"].values.tolist()\n",
    "    labels = df[\"tag\"].values\n",
    "    labels = label_processor(labels, LABELS_CLASS)\n",
    "    labels = keras.utils.to_categorical(labels, NUM_CLASSES)\n",
    "    \n",
    "    video_batch = np.zeros((num_samples, MAX_SEQ_LENGTH, 6912), dtype=\"float32\")\n",
    "\n",
    "    # For each video.\n",
    "    \n",
    "    for idx, path in enumerate(video_paths):\n",
    "        # Gather all its frames and add a batch dimension.\n",
    "        frames = load_video(os.path.join(root_dir, path))\n",
    "        frames = frames[None, ...]\n",
    "        \n",
    "        # Extract features from the frames of the current video.\n",
    "        for i, batch in enumerate(frames):\n",
    "            video_length = batch.shape[0]\n",
    "            select_frame = np.linspace(0, video_length-1, MAX_SEQ_LENGTH,endpoint=True,retstep=True,dtype=int)[0]\n",
    "            # length = min(MAX_SEQ_LENGTH, video_length)\n",
    "            \n",
    "            video_batch[idx] = batch[select_frame].reshape(20, 6912).astype('float32') / 255\n",
    "\n",
    "    return video_batch, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "79832568",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frame features in train set: (20, 6912)\n"
     ]
    }
   ],
   "source": [
    "train_data, train_labels = prepare_all_videos(train_df, \"C:/nyu/DRL/final_project/dataset/UCF101/train\")\n",
    "print(f\"Frame features in train set: {train_data[0].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c3a87197",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    loss=\"categorical_crossentropy\", optimizer=tf.keras.optimizers.Adam(), metrics=[\"accuracy\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fec16533",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\irohc\\anaconda3\\envs\\drl_proj\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/20\n",
      "594/594 [==============================] - 20s 34ms/sample - loss: 1.5830 - acc: 0.3266\n",
      "Epoch 2/20\n",
      "594/594 [==============================] - 20s 34ms/sample - loss: 1.4038 - acc: 0.4781\n",
      "Epoch 3/20\n",
      "594/594 [==============================] - 20s 33ms/sample - loss: 1.2235 - acc: 0.5572\n",
      "Epoch 4/20\n",
      "594/594 [==============================] - 18s 30ms/sample - loss: 1.0885 - acc: 0.6414\n",
      "Epoch 5/20\n",
      "594/594 [==============================] - 22s 36ms/sample - loss: 1.0078 - acc: 0.6768\n",
      "Epoch 6/20\n",
      "594/594 [==============================] - 20s 34ms/sample - loss: 0.9436 - acc: 0.6869\n",
      "Epoch 7/20\n",
      "594/594 [==============================] - 19s 32ms/sample - loss: 0.8672 - acc: 0.7290\n",
      "Epoch 8/20\n",
      "594/594 [==============================] - 15s 24ms/sample - loss: 0.8490 - acc: 0.7222\n",
      "Epoch 9/20\n",
      "594/594 [==============================] - 12s 20ms/sample - loss: 0.7785 - acc: 0.7525\n",
      "Epoch 10/20\n",
      "594/594 [==============================] - 14s 23ms/sample - loss: 0.7530 - acc: 0.7576\n",
      "Epoch 11/20\n",
      "594/594 [==============================] - 14s 23ms/sample - loss: 0.7015 - acc: 0.7778\n",
      "Epoch 12/20\n",
      "594/594 [==============================] - 13s 23ms/sample - loss: 0.6842 - acc: 0.7778\n",
      "Epoch 13/20\n",
      "594/594 [==============================] - 12s 21ms/sample - loss: 0.6863 - acc: 0.7828\n",
      "Epoch 14/20\n",
      "594/594 [==============================] - 13s 22ms/sample - loss: 0.6405 - acc: 0.7946\n",
      "Epoch 15/20\n",
      "594/594 [==============================] - 13s 22ms/sample - loss: 0.7072 - acc: 0.7677\n",
      "Epoch 16/20\n",
      "594/594 [==============================] - 12s 21ms/sample - loss: 0.6482 - acc: 0.7795s -\n",
      "Epoch 17/20\n",
      "594/594 [==============================] - 13s 21ms/sample - loss: 0.5661 - acc: 0.8266\n",
      "Epoch 18/20\n",
      "594/594 [==============================] - 14s 23ms/sample - loss: 0.5356 - acc: 0.8367\n",
      "Epoch 19/20\n",
      "594/594 [==============================] - 13s 21ms/sample - loss: 0.5569 - acc: 0.8300\n",
      "Epoch 20/20\n",
      "594/594 [==============================] - 14s 24ms/sample - loss: 0.5470 - acc: 0.8283\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x25b793da988>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_data, train_labels, shuffle=True,\n",
    "      batch_size=10, epochs=20,\n",
    "      verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "847d9832",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drl_proj",
   "language": "python",
   "name": "drl_proj"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
